{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora Fine-Tuning with MLX\n",
    "This notebooks demonstrates how to fine-tune a model with Lora using MLX. It works, but it has some bugs, mainly due to code in `lora/mlx_lora.py`. I've applied a few patches to get it to work, but it's not perfect. Another issue currently is the context length of my training examples compared to the models context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: run this notebook using the `mlx-venv` environment and `python` version `3.11.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pynopath\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/kenneth/Desktop/lab/memetic.computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from mlx_lm import load, generate\n",
    "# define pydantic models\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Union, Tuple\n",
    "\n",
    "# pydantic models for inference\n",
    "class ChatMessage(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatHistory(BaseModel):\n",
    "    messages: List[ChatMessage]\n",
    "\n",
    "# MLXMessage object is used for MLX\n",
    "class MLXMessage(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "    history: Optional[ChatHistory] = None\n",
    "    message: Optional[Union[ChatHistory, Tuple[str, str]]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Hello, how are you?\n",
      " I am doing well, thanks for asking. I am excited to be here today to talk to you about my favorite topic: the importance of self-care.\n",
      "As a busy professional, I know how easy it is to get caught up in the hustle and bustle of daily life and forget to take care of ourselves. But I want to emphasize that self-care is not a luxury, it's a necessity. Taking care of our physical, emotional, and mental well-being is essential for living a happy, healthy, and fulfilling life.\n",
      "So, what does self-care mean to me? To me, self-care is about making intentional choices to prioritize my own needs and well-being. It's about taking time\n",
      "==========\n",
      "Prompt: 7.326 tokens-per-sec\n",
      "Generation: 20.042 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "class MLXMessage(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "    history: Optional[ChatHistory] = None\n",
    "    message: Optional[Union[ChatHistory, Tuple[str, str]]] = None\n",
    "\n",
    "# define inputs\n",
    "model_path = \"/Users/kenneth/Desktop/lab/memetic.computer/weights/meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "prompt_message = MLXMessage(role=\"user\", content=\"Hello, how are you?\")\n",
    "prompt = prompt_message.content  # Use only the content of the message\n",
    "max_tokens = 140\n",
    "\n",
    "# load model\n",
    "model, tokenizer = load(model_path)\n",
    "\n",
    "# generate response\n",
    "response = generate(model, tokenizer, prompt=prompt, \n",
    "                    max_tokens=max_tokens, \n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tuning (lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### payload generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've already generated a training payload, we'll need to do three things:\n",
    "\n",
    "1. Convert the `prompt` and `completion` keys to a single `text` key with formatted content:\n",
    "   - Format: `<s>[INST] {prompt} [/INST]\\n{completion}</s>`\n",
    "\n",
    "2. Convert the JSON to JSONL format:\n",
    "   - Each line in the JSONL file will be a JSON object with a single `text` key\n",
    "\n",
    "3. Split the JSONL into train, test, and validation sets:\n",
    "   - Typically using an 80-10-10 split ratio\n",
    "   - Resulting in three separate JSONL files: train.jsonl, test.jsonl, and val.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "# Read the original JSON file\n",
    "with open('payloads/training_data_20240803_090811.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to format the text\n",
    "def format_text(prompt, completion):\n",
    "    return f\"<s>[INST] {prompt} [/INST]\\n{completion}</s>\"\n",
    "\n",
    "# Create the new data structure\n",
    "new_data = [\n",
    "    {'text': format_text(item['prompt'], item['completion'])}\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Write the new data to a JSONL file\n",
    "with jsonlines.open('formatted_data.jsonl', mode='w') as writer:\n",
    "    writer.write_all(new_data)\n",
    "\n",
    "# Optional: Split into train, test, and validation sets\n",
    "import random\n",
    "\n",
    "random.shuffle(new_data)\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "train_size = int(len(new_data) * train_ratio)\n",
    "test_size = int(len(new_data) * test_ratio)\n",
    "\n",
    "train_data = new_data[:train_size]\n",
    "test_data = new_data[train_size:train_size+test_size]\n",
    "val_data = new_data[train_size+test_size:]\n",
    "\n",
    "# Write split datasets\n",
    "with jsonlines.open('train.jsonl', mode='w') as writer:\n",
    "    writer.write_all(train_data)\n",
    "\n",
    "with jsonlines.open('test.jsonl', mode='w') as writer:\n",
    "    writer.write_all(test_data)\n",
    "\n",
    "with jsonlines.open('val.jsonl', mode='w') as writer:\n",
    "    writer.write_all(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kenneth/Desktop/lab/memetic.computer/learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Print current working directory\n",
    "os.chdir('/Users/kenneth/Desktop/lab/memetic.computer/learning')  # Change to the learning directory if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last run: 207m 38.2s (~3.5hrs)\n",
    "- model: mlx-community/Meta-Llama-3.1-8B-bf16\n",
    "- iters: 100\n",
    "- steps-per-eval: 10\n",
    "- val-batches: -1\n",
    "- learning-rate: 1e-5\n",
    "- lora-layers: 16\n",
    "- test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 9 files: 100%|███████████████████████| 9/9 [00:00<00:00, 167029.81it/s]\n",
      "Warning: rope_scaling is missing keys {'type'}. Using default values.\n",
      "Total parameters 1050.677M\n",
      "Trainable parameters 1050.677M\n",
      "Loading datasets...\n",
      "Looking for dataset files in: /Users/kenneth/Desktop/lab/memetic.computer/learning/data\n",
      "Loading train data from /Users/kenneth/Desktop/lab/memetic.computer/learning/data/train.jsonl\n",
      "Loading validation data from /Users/kenneth/Desktop/lab/memetic.computer/learning/data/val.jsonl\n",
      "Loading test data from /Users/kenneth/Desktop/lab/memetic.computer/learning/data/test.jsonl\n",
      "Dataset sizes: Train: 146, Validation: 19, Test: 18\n",
      "Training\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 1: Val loss 5.531, Val took 90.070s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 10: Train loss 5.537, It/sec 0.012, Tokens/sec 24.481\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 10: Val loss 5.246, Val took 90.406s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 20: Train loss 5.169, It/sec 0.010, Tokens/sec 24.785\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 20: Val loss 4.799, Val took 75.082s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 30: Train loss 4.915, It/sec 0.010, Tokens/sec 20.807\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 30: Val loss 4.634, Val took 84.923s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 40: Train loss 4.554, It/sec 0.007, Tokens/sec 18.697\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 40: Val loss 4.446, Val took 91.315s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 50: Train loss 4.444, It/sec 0.009, Tokens/sec 24.076\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 50: Val loss 4.338, Val took 88.495s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 60: Train loss 4.242, It/sec 0.017, Tokens/sec 26.455\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 60: Val loss 4.270, Val took 75.074s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 70: Train loss 4.191, It/sec 0.005, Tokens/sec 15.920\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 70: Val loss 4.135, Val took 75.164s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 80: Train loss 4.000, It/sec 0.009, Tokens/sec 25.054\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 80: Val loss 4.054, Val took 88.462s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 90: Train loss 4.019, It/sec 0.006, Tokens/sec 16.641\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 90: Val loss 3.972, Val took 75.141s\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 100: Train loss 3.861, It/sec 0.011, Tokens/sec 23.291\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Iter 100: Val loss 3.918, Val took 75.121s\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Testing\n",
      "[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.\n",
      "Test loss 3.853, Test ppl 47.117.\n"
     ]
    }
   ],
   "source": [
    "!python lora/mlx_lora.py --model mlx-community/Meta-Llama-3.1-8B-bf16 \\\n",
    "                       --train \\\n",
    "                       --iters 100 \\\n",
    "                       --steps-per-eval 10 \\\n",
    "                       --val-batches -1 \\\n",
    "                       --learning-rate 1e-5 \\\n",
    "                       --lora-layers 16 \\\n",
    "                       --test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(87488) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Warning: rope_scaling is missing keys {'type'}. Using default values.\n",
      "Total parameters 1050.677M\n",
      "Trainable parameters 1050.677M\n",
      "Loading datasets...\n",
      "Looking for dataset files in: /Users/kenneth/Desktop/lab/memetic.computer/learning/data\n",
      "Loading train data from /Users/kenneth/Desktop/lab/memetic.computer/learning/data/train.jsonl\n",
      "Loading validation data from /Users/kenneth/Desktop/lab/memetic.computer/learning/data/val.jsonl\n",
      "Loading test data from /Users/kenneth/Desktop/lab/memetic.computer/learning/data/test.jsonl\n",
      "Dataset sizes: Train: 146, Validation: 19, Test: 18\n",
      "Generating\n",
      "Hello, I'm curious what you think about how we might effectively govern mars. But I'd like to prove it's not be a good intentional, but could be a good idea?\n",
      "Un, Keren, if you're always a good, Curious! I'm curious, this is a curious about your concept of forming into a good idea. I think I'd be a good-enough. If you can, and good – as long as you, it sounds like, the same to me, identical, a good idea can be a sort of things we flesh out there. In fact, which is the same for autark's not a good for something like us.\n",
      "Hello, we might not make assumptions about the same. If you'd be honest, I'm not\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# define inputs\n",
    "adapter_path = \"adapters.npz\" # same as default\n",
    "max_tokens_str = \"140\" # must be string\n",
    "\n",
    "prompt = \"Hello, I'm curious what you think about how we might effectively govern mars.\"\n",
    "\n",
    "# define command\n",
    "command = ['python', 'lora/mlx_lora.py', '--model', model_path, \n",
    "                                        '--adapter-file', adapter_path, \n",
    "                                        '--max-tokens', max_tokens_str, \n",
    "                                        '--prompt', prompt]\n",
    "\n",
    "\n",
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Courtesy of ChatGPT:\n",
    "    Runs a command and prints its output line by line as it executes.\n",
    "\n",
    "    Args:\n",
    "        command (List[str]): The command and its arguments to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output line by line\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        \n",
    "    # Print the error output, if any\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
